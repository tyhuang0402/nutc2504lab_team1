# ------------------------
# poc-manifest.yaml
# ------------------------
# 1) Namespaces
apiVersion: v1
kind: Namespace
metadata:
  name: tenant-a
---
apiVersion: v1
kind: Namespace
metadata:
  name: tenant-b
---
# 2) ResourceQuotas (模擬包含 nvidia.com/gpu)
apiVersion: v1
kind: ResourceQuota
metadata:
  name: tenant-a-quota
  namespace: tenant-a
spec:
  hard:
    pods: "5"
    requests.cpu: "2"
    requests.memory: 4Gi
    limits.memory: 6Gi
    requests.nvidia.com/gpu: "1"
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: tenant-b-quota
  namespace: tenant-b
spec:
  hard:
    pods: "3"
    requests.cpu: "1"
    requests.memory: 2Gi
    limits.memory: 3Gi
    requests.nvidia.com/gpu: "1"
---
# 3) llm-stub (簡單 HTTP server that triggers metrics-emitter)
apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-stub-cm
  namespace: default
data:
  server.py: |
    from http.server import BaseHTTPRequestHandler, HTTPServer
    import requests, json
    class H(BaseHTTPRequestHandler):
        def do_POST(self):
            length = int(self.headers.get('content-length',0))
            body = self.rfile.read(length).decode()
            api_key = self.headers.get('x-api-key','unknown')
            try:
                requests.post("http://metrics-emitter.default/emit", json={"api_key": api_key}, timeout=2)
            except Exception as e:
                pass
            self.send_response(200)
            self.send_header('Content-type','application/json')
            self.end_headers()
            self.wfile.write(json.dumps({"reply":"ok","api_key":api_key}).encode())
    if __name__=='__main__':
        server=HTTPServer(('0.0.0.0',8080),H)
        server.serve_forever()
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-stub
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llm-stub
  template:
    metadata:
      labels:
        app: llm-stub
    spec:
      containers:
      - name: llm-stub
        image: python:3.11-slim
        command: ["python","-u","/app/server.py"]
        volumeMounts:
        - name: app
          mountPath: /app
      volumes:
      - name: app
        configMap:
          name: llm-stub-cm
---
apiVersion: v1
kind: Service
metadata:
  name: llm-stub
  namespace: default
spec:
  selector:
    app: llm-stub
  ports:
  - protocol: TCP
    port: 8080
    targetPort: 8080
---
# 4) mock metrics-emitter (exposes Prometheus metrics like dcgm_* and accepts POST /emit)
apiVersion: v1
kind: ConfigMap
metadata:
  name: metrics-emitter-cm
  namespace: default
data:
  emitter.py: |
    from prometheus_client import start_http_server, Gauge
    from flask import Flask, request
    import threading, time, random
    app = Flask(__name__)
    g_util = Gauge('dcgm_gpu_utilization_percentage', 'GPU util %', ['api_key','tenant'])
    g_fb = Gauge('dcgm_fb_used_bytes', 'GPU fb used bytes', ['api_key','tenant'])
    @app.route('/emit', methods=['POST'])
    def emit():
        j = request.json or {}
        api_key = j.get('api_key','unknown')
        tenant = 'tenant-a' if api_key=='key-a' else 'tenant-b' if api_key=='key-b' else 'unknown'
        util = random.randint(5,90)
        fb = random.randint(50000000,800000000)
        g_util.labels(api_key=api_key, tenant=tenant).set(util)
        g_fb.labels(api_key=api_key, tenant=tenant).set(fb)
        return {'status':'ok'}
    if __name__=='__main__':
        threading.Thread(target=lambda: start_http_server(9100), daemon=True).start()
        app.run(host='0.0.0.0', port=8081)
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metrics-emitter
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: metrics-emitter
  template:
    metadata:
      labels:
        app: metrics-emitter
    spec:
      containers:
      - name: metrics
        image: python:3.11-slim
        command: ["python","-u","/app/emitter.py"]
        volumeMounts:
        - name: app
          mountPath: /app
      volumes:
      - name: app
        configMap:
          name: metrics-emitter-cm
---
apiVersion: v1
kind: Service
metadata:
  name: metrics-emitter
  namespace: default
spec:
  selector:
    app: metrics-emitter
  ports:
  - name: metrics
    port: 9100
    targetPort: 9100
---
# 5) Prometheus ServiceMonitor (讓 kube-prometheus 抓 metrics-emitter)
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: metrics-emitter-sm
  namespace: monitoring
  labels:
    release: monitoring
spec:
  selector:
    matchLabels:
      app: metrics-emitter
  namespaceSelector:
    matchNames:
    - default
  endpoints:
  - port: metrics
    interval: 15s
---
# 6) Simple NetworkPolicy for tenant namespaces to prevent ingress from other namespaces
#    (k8s NetworkPolicy; 如果你安裝了 Cilium，Cilium 也會尊重這些或可以換成 CiliumNetworkPolicy)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-cross-namespace-ingress
  namespace: tenant-a
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector: {}   # allow from same namespace
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-cross-namespace-ingress
  namespace: tenant-b
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector: {}
---
# 7) Kong CRD examples (KongConsumer + KongPlugin + Ingress)
#    NOTE: Kong CRDs must be available (installed by Kong helm chart). 
apiVersion: configuration.konghq.com/v1
kind: KongConsumer
metadata:
  name: consumer-key-a
  namespace: default
username: tenant-a-consumer
---
apiVersion: configuration.konghq.com/v1
kind: KongCredential
metadata:
  name: keyauth-key-a
  namespace: default
consumerRef: tenant-a-consumer
type: key-auth
config:
  key: key-a
---
apiVersion: configuration.konghq.com/v1
kind: KongConsumer
metadata:
  name: consumer-key-b
  namespace: default
username: tenant-b-consumer
---
apiVersion: configuration.konghq.com/v1
kind: KongCredential
metadata:
  name: keyauth-key-b
  namespace: default
consumerRef: tenant-b-consumer
type: key-auth
config:
  key: key-b
---
apiVersion: configuration.konghq.com/v1
kind: KongPlugin
metadata:
  name: rate-limit-default
  namespace: default
plugin: rate-limiting
config:
  minute: 20
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: llm-ingress
  namespace: default
  annotations:
    konghq.com/plugins: rate-limit-default
spec:
  rules:
  - http:
      paths:
      - path: /v1/llm
        pathType: Prefix
        backend:
          service:
            name: llm-stub
            port:
              number: 8080
